A2PM
====


Introduction
------------

This package contains the A2PM, a Machine Learning/Rejuvenation Toolkit,
which can be used to proactively predict the Remaining Time to Crash (RTTC) of
virtualized web-based application, and use the models to proactively
rejuvenate virtual machines when the crashing point is approaching,
during the system's operation, so as to prevent possible system failures.

Specifically, with an estimation of the RTTC of a component of the system 
under prediction mode, a proactive action for rejuvenating the system 
(e.g. via early restart of a machine) is performed before the component fails
to reduce, as much as possible, the system unavailability.

The input of the prediction model is a set of variables which are monitored
at runtime and related to utilization of system resources (CPU, memory, active threads, ...).
The actual prediction model is built using offline training using resource data
collected during the training phase.

To speed up the model generation time, the software uses regularization techniques 
to reduce the number of input parameters to the machine learner.



Architecture of the Toolkit
---------------------------

The toolkit is composed of three different software packages,
which are related to the respective Framework's execution stage.

Since the Framework relies on offline learning, the first step
is to collect hardware usage statistics from the involved machine.
This can be done using the tools in the `feature_collection/`
subfolder. A client/server architecture is provided there.

Utilities to generate artificial memory leaks and unterminated
threads are provided in `utilities/`, in order to let the monitored
system to crash more quickly, provided that a specific injection
rate of anomalies is a-priori known.

The actual generation of the model is done by the toolkit
in `model_build/` and can be run on a machine different from
the one(s) involved in the collection of data.

Finally, once the model has been built, the actual online prediction
and rejuvenation can be enforced by the tools in `controller/`. 




Installation Notes
------------------

### Dependencies

Most of the tools just require a standard C compiler on a POSIX
system, with pthreads. Any Linux distribution will likely meet
these dependencies.

On the other hand, concerning the ML Framework, an additional set
of dependecies are required, namely:
  - gnuplot (to generate plots of the prediction models)
  - WEKA (at least version 3.7, to run some of the learning algorithm)
  - Matlab (to run data regularization, and to build the Lasso prediction model)
  - LaTeX (for automatic generation of training reports)
  
Failing to satisfy any of these dependencies will not allow the
ML Framework to work properly, or will prevent some steps to
be correctly carried out. Some of these steps can be nevertheless
disabled during the configuration of the framework. The corresponding
steps can be activated as well depending on the dependencies which
are met on a specific machine.


### Compilation and Configuration

To compile the tools, simply issue `make` in the main folder
of the package.

The only additional installation step requires manually
configuring the `model_build/FRAMEWORK.sh` script. Several
variables should be set according to the following meaning.

  - `database_file`: this is the database file generated by the feature
					 collection architecture keeping the data obtained from
					 the machines under monitoring when injecting anomalies
					 and making them crash. By default, the output name
					 of the file is db.txt, but here a different name
					 can be specified for convenience.
  - `lambdas`: this is the set of lambdas used to build prediction models
			   with Lasso. Defaults range from very small values to very
			   large ones, allowing to extensively test the behaviour of
			   Lasso
  - `WEKA_PATH`: this must be set to the path where Weka is installed on
			     the system. If this variable is not correctly set, the steps
				 `run_weka_*` should be later disabled.
  - `lasso_algorithm`: this specified which Lasso variant should be used
					   when building models using Lasso. Supported variants
					   are `grafting`, `iteratedRidge`, `nonNegativeSquared`, and
					   `shooting`.

Then, the steps to be carried out by the Framework can be specified
with a set of additional variables. Setting a variable to `false`
disables the step, while setting it to `true` enables it. The available
steps are>

  - `purge_old_data`: this step cleans out all results from previous
						executions of the framework
  - `run_datapoint_aggregation`: this step generates aggregated datapoints
						from the initial database file, which can be later
						used for training. The Framework relies on aggregated
						datapoints, so this step is mandatory, and could be
						disabled only if it has been run at least once.
  - `run_lasso`: this step runs the selected Lasso algorithm for all the
				 specified lambda values.
  - `apply_lasso`: this step builds datasets in the `csv/` subfolder according
				   to the weights computed by Lasso for each lambda value. These
				   files are needed only for manual inspection, or to plot the
				   prediction model plots.
  - `plot_original_parameters`: this step generates plots of the trend of original parameters
								and the trend of parameters selected by Lasso.
  - `plot_lasso_as_predictor`: this step plots prediction models built using Lasso.
  - `run_weka_linear`: this step builds the Linear Regression model using WEKA and
					   plots the prediction model built.
  - `run_weka_m5p`: this step builds the M5P model using WEKA and
					   plots the prediction model built.
  - `run_weka_REPtree`: this step builds the REP Tree model using WEKA and
					   plots the prediction model built.
  - `run_weka_svm`: this step builds the SVM model using WEKA and
					   plots the prediction model built.
  - `run_weka_svm2`: this step builds the SVM2 model using WEKA and
					   plots the prediction model built.
  - `evaluate_error`: this steps computes the error of all prediction models
  - `generate_report`: this final steps builds the report with all the information
					   obtained. This requires LaTeX and to have run all the previous
					   steps.


### Usage

Collecting data is quite automatic: simply launch the client
on the machine which we want to monitor and the server on
the machine where the data is to be collected.

The controller usage is perfectly similar to that of the
collection utility.

To inject anomalies, simply run either `leaks` or `threads`
in background.

The ML Framework requires quite a lot of steps to complete
its job. We provide in the `model_build/` folder the convenience
FRAMEWORK.sh script which executes all the required steps.
To execute only some steps, you can set the variables
corresponding to the steps to exclude at the beginning of
the script to `false`.
